{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "225450b5",
   "metadata": {},
   "source": [
    "# Deep Learning Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32750b2",
   "metadata": {},
   "source": [
    "## Reading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0078f40b",
   "metadata": {},
   "source": [
    "Import the `pandas` package, then use the `read_csv` function to read the labeled training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ba62a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 3)\n",
      "['id' 'sentiment' 'review']\n",
      "\"With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the labeled training data\n",
    "train = pd.read_csv(\"../data/labeled_train_data.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "print(train.shape)\n",
    "print(train.columns.values)\n",
    "print(train[\"review\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d20d8fb",
   "metadata": {},
   "source": [
    "## Data Cleaning and Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acdddb7",
   "metadata": {},
   "source": [
    "Import the Beautiful Soup library. Remove HTML markup from review text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bf5b6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw review text:\n",
      " \"With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"\n",
      "Modified review text:\n",
      " \"With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Initialize the BeautifulSoup object on a single review\n",
    "example_review = BeautifulSoup(train[\"review\"][0])\n",
    "\n",
    "# Print the raw text and the text without tags or markup\n",
    "print(\"Raw review text:\\n\", train[\"review\"][0])\n",
    "print(\"Modified review text:\\n\", example_review.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5a6c3d",
   "metadata": {},
   "source": [
    "For simplicity, remove all punctuation and numbers. However, in sentiment analysis problems, it is important to remember that punctuation and numbers often carry sentiment and should usually be treated as words.\n",
    "\n",
    "To remove punctuation and numbers, use regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "321f2a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text after removing punctuation and numbers:\n",
      "  With all this stuff going down at the moment with MJ i ve started listening to his music  watching the odd documentary here and there  watched The Wiz and watched Moonwalker again  Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent  Moonwalker is part biography  part feature film which i remember going to see at the cinema when it was originally released  Some of it has subtle messages about MJ s feeling towards the press and also the obvious message of drugs are bad m kay Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring  Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him The actual feature film bit when it finally starts is only on for    minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord  Why he wants MJ dead so bad is beyond me  Because MJ overheard his plans  Nah  Joe Pesci s character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno  maybe he just hates MJ s music Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence  Also  the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene Bottom line  this movie is for people who like MJ on one level or another  which i think is most people   If not  then stay away  It does try and give off a wholesome message and ironically MJ s bestest buddy in this movie is a girl  Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty  Well  with all the attention i ve gave this subject    hmmm well i don t know because people can be different behind closed doors  i know this for a fact  He is either an extremely nice but stupid guy or one of the most sickest liars  I hope he is not the latter  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Use regular expressions to do a find and-replace\n",
    "letters_only = re.sub(\"[^a-zA-Z]\", \" \", example_review.get_text())\n",
    "\n",
    "print(\"Text after removing punctuation and numbers:\\n\", letters_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a513be1",
   "metadata": {},
   "source": [
    "Convert reviews to lower_case and split them into individual words (\"tokenization\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bf7c4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower-case tokens:\n",
      " ['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again', 'maybe', 'i', 'just', 'want', 'to', 'get', 'a', 'certain', 'insight', 'into', 'this', 'guy', 'who', 'i', 'thought', 'was', 'really', 'cool', 'in', 'the', 'eighties', 'just', 'to', 'maybe', 'make', 'up', 'my', 'mind', 'whether', 'he', 'is', 'guilty', 'or', 'innocent', 'moonwalker', 'is', 'part', 'biography', 'part', 'feature', 'film', 'which', 'i', 'remember', 'going', 'to', 'see', 'at', 'the', 'cinema', 'when', 'it', 'was', 'originally', 'released', 'some', 'of', 'it', 'has', 'subtle', 'messages', 'about', 'mj', 's', 'feeling', 'towards', 'the', 'press', 'and', 'also', 'the', 'obvious', 'message', 'of', 'drugs', 'are', 'bad', 'm', 'kay', 'visually', 'impressive', 'but', 'of', 'course', 'this', 'is', 'all', 'about', 'michael', 'jackson', 'so', 'unless', 'you', 'remotely', 'like', 'mj', 'in', 'anyway', 'then', 'you', 'are', 'going', 'to', 'hate', 'this', 'and', 'find', 'it', 'boring', 'some', 'may', 'call', 'mj', 'an', 'egotist', 'for', 'consenting', 'to', 'the', 'making', 'of', 'this', 'movie', 'but', 'mj', 'and', 'most', 'of', 'his', 'fans', 'would', 'say', 'that', 'he', 'made', 'it', 'for', 'the', 'fans', 'which', 'if', 'true', 'is', 'really', 'nice', 'of', 'him', 'the', 'actual', 'feature', 'film', 'bit', 'when', 'it', 'finally', 'starts', 'is', 'only', 'on', 'for', 'minutes', 'or', 'so', 'excluding', 'the', 'smooth', 'criminal', 'sequence', 'and', 'joe', 'pesci', 'is', 'convincing', 'as', 'a', 'psychopathic', 'all', 'powerful', 'drug', 'lord', 'why', 'he', 'wants', 'mj', 'dead', 'so', 'bad', 'is', 'beyond', 'me', 'because', 'mj', 'overheard', 'his', 'plans', 'nah', 'joe', 'pesci', 's', 'character', 'ranted', 'that', 'he', 'wanted', 'people', 'to', 'know', 'it', 'is', 'he', 'who', 'is', 'supplying', 'drugs', 'etc', 'so', 'i', 'dunno', 'maybe', 'he', 'just', 'hates', 'mj', 's', 'music', 'lots', 'of', 'cool', 'things', 'in', 'this', 'like', 'mj', 'turning', 'into', 'a', 'car', 'and', 'a', 'robot', 'and', 'the', 'whole', 'speed', 'demon', 'sequence', 'also', 'the', 'director', 'must', 'have', 'had', 'the', 'patience', 'of', 'a', 'saint', 'when', 'it', 'came', 'to', 'filming', 'the', 'kiddy', 'bad', 'sequence', 'as', 'usually', 'directors', 'hate', 'working', 'with', 'one', 'kid', 'let', 'alone', 'a', 'whole', 'bunch', 'of', 'them', 'performing', 'a', 'complex', 'dance', 'scene', 'bottom', 'line', 'this', 'movie', 'is', 'for', 'people', 'who', 'like', 'mj', 'on', 'one', 'level', 'or', 'another', 'which', 'i', 'think', 'is', 'most', 'people', 'if', 'not', 'then', 'stay', 'away', 'it', 'does', 'try', 'and', 'give', 'off', 'a', 'wholesome', 'message', 'and', 'ironically', 'mj', 's', 'bestest', 'buddy', 'in', 'this', 'movie', 'is', 'a', 'girl', 'michael', 'jackson', 'is', 'truly', 'one', 'of', 'the', 'most', 'talented', 'people', 'ever', 'to', 'grace', 'this', 'planet', 'but', 'is', 'he', 'guilty', 'well', 'with', 'all', 'the', 'attention', 'i', 've', 'gave', 'this', 'subject', 'hmmm', 'well', 'i', 'don', 't', 'know', 'because', 'people', 'can', 'be', 'different', 'behind', 'closed', 'doors', 'i', 'know', 'this', 'for', 'a', 'fact', 'he', 'is', 'either', 'an', 'extremely', 'nice', 'but', 'stupid', 'guy', 'or', 'one', 'of', 'the', 'most', 'sickest', 'liars', 'i', 'hope', 'he', 'is', 'not', 'the', 'latter']\n"
     ]
    }
   ],
   "source": [
    "# Convert to lower-case and split into words\n",
    "lower_case = letters_only.lower()\n",
    "tokens = lower_case.split()\n",
    "\n",
    "print(\"Lower-case tokens:\\n\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e06022",
   "metadata": {},
   "source": [
    "Decide how to deal with frequently occurring words that don't carry much meaning (\"stop words\"), such as \"a\", \"and\", \"is\", and \"the\".\n",
    "\n",
    "Import a stop word list from the Python Natural Language Toolkit and remove stop words from the review text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a78d039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words in NLTK: ['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n",
      "Tokens after removing stop words:\n",
      " ['stuff', 'going', 'moment', 'mj', 'started', 'listening', 'music', 'watching', 'odd', 'documentary', 'watched', 'wiz', 'watched', 'moonwalker', 'maybe', 'want', 'get', 'certain', 'insight', 'guy', 'thought', 'really', 'cool', 'eighties', 'maybe', 'make', 'mind', 'whether', 'guilty', 'innocent', 'moonwalker', 'part', 'biography', 'part', 'feature', 'film', 'remember', 'going', 'see', 'cinema', 'originally', 'released', 'subtle', 'messages', 'mj', 'feeling', 'towards', 'press', 'also', 'obvious', 'message', 'drugs', 'bad', 'kay', 'visually', 'impressive', 'course', 'michael', 'jackson', 'unless', 'remotely', 'like', 'mj', 'anyway', 'going', 'hate', 'find', 'boring', 'may', 'call', 'mj', 'egotist', 'consenting', 'making', 'movie', 'mj', 'fans', 'would', 'say', 'made', 'fans', 'true', 'really', 'nice', 'actual', 'feature', 'film', 'bit', 'finally', 'starts', 'minutes', 'excluding', 'smooth', 'criminal', 'sequence', 'joe', 'pesci', 'convincing', 'psychopathic', 'powerful', 'drug', 'lord', 'wants', 'mj', 'dead', 'bad', 'beyond', 'mj', 'overheard', 'plans', 'nah', 'joe', 'pesci', 'character', 'ranted', 'wanted', 'people', 'know', 'supplying', 'drugs', 'etc', 'dunno', 'maybe', 'hates', 'mj', 'music', 'lots', 'cool', 'things', 'like', 'mj', 'turning', 'car', 'robot', 'whole', 'speed', 'demon', 'sequence', 'also', 'director', 'must', 'patience', 'saint', 'came', 'filming', 'kiddy', 'bad', 'sequence', 'usually', 'directors', 'hate', 'working', 'one', 'kid', 'let', 'alone', 'whole', 'bunch', 'performing', 'complex', 'dance', 'scene', 'bottom', 'line', 'movie', 'people', 'like', 'mj', 'one', 'level', 'another', 'think', 'people', 'stay', 'away', 'try', 'give', 'wholesome', 'message', 'ironically', 'mj', 'bestest', 'buddy', 'movie', 'girl', 'michael', 'jackson', 'truly', 'one', 'talented', 'people', 'ever', 'grace', 'planet', 'guilty', 'well', 'attention', 'gave', 'subject', 'hmmm', 'well', 'know', 'people', 'different', 'behind', 'closed', 'doors', 'know', 'fact', 'either', 'extremely', 'nice', 'stupid', 'guy', 'one', 'sickest', 'liars', 'hope', 'latter']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rohanmistry/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "print(\"Stop words in NLTK:\", stopwords.words(\"english\"))\n",
    "\n",
    "# Remove stop words from the review text\n",
    "tokens = [token for token in tokens if token not in stopwords.words(\"english\")]\n",
    "print(\"Tokens after removing stop words:\\n\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962041f5",
   "metadata": {},
   "source": [
    "Create reusable function to clean review text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0bc92a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_review_text(review: str) -> list:\n",
    "    \"\"\"\n",
    "    Cleans the review text by removing HTML tags, markup, punctuation, numbers, and stop words.\n",
    "\n",
    "    Parameters:\n",
    "        review (str): The raw review text to be cleaned.\n",
    "    \n",
    "    Returns: \n",
    "        list: A list of cleaned tokens from the review.\n",
    "    \"\"\"\n",
    "\n",
    "    text = BeautifulSoup(review).get_text()\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    tokens = letters_only.lower().split()\n",
    "    tokens = [token for token in tokens if token not in stops]\n",
    "\n",
    "    return (\" \".join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9c7493c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned review text:\n",
      " stuff going moment mj started listening music watching odd documentary watched wiz watched moonwalker maybe want get certain insight guy thought really cool eighties maybe make mind whether guilty innocent moonwalker part biography part feature film remember going see cinema originally released subtle messages mj feeling towards press also obvious message drugs bad kay visually impressive course michael jackson unless remotely like mj anyway going hate find boring may call mj egotist consenting making movie mj fans would say made fans true really nice actual feature film bit finally starts minutes excluding smooth criminal sequence joe pesci convincing psychopathic powerful drug lord wants mj dead bad beyond mj overheard plans nah joe pesci character ranted wanted people know supplying drugs etc dunno maybe hates mj music lots cool things like mj turning car robot whole speed demon sequence also director must patience saint came filming kiddy bad sequence usually directors hate working one kid let alone whole bunch performing complex dance scene bottom line movie people like mj one level another think people stay away try give wholesome message ironically mj bestest buddy movie girl michael jackson truly one talented people ever grace planet guilty well attention gave subject hmmm well know people different behind closed doors know fact either extremely nice stupid guy one sickest liars hope latter\n"
     ]
    }
   ],
   "source": [
    "clean_review = clean_review_text(train[\"review\"][0])\n",
    "print(\"Cleaned review text:\\n\", clean_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1910fdf",
   "metadata": {},
   "source": [
    "Loop through and clean entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4f8e856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning review 1000 of 25000\n",
      "Cleaning review 2000 of 25000\n",
      "Cleaning review 3000 of 25000\n",
      "Cleaning review 4000 of 25000\n",
      "Cleaning review 5000 of 25000\n",
      "Cleaning review 6000 of 25000\n",
      "Cleaning review 7000 of 25000\n",
      "Cleaning review 8000 of 25000\n",
      "Cleaning review 9000 of 25000\n",
      "Cleaning review 10000 of 25000\n",
      "Cleaning review 11000 of 25000\n",
      "Cleaning review 12000 of 25000\n",
      "Cleaning review 13000 of 25000\n",
      "Cleaning review 14000 of 25000\n",
      "Cleaning review 15000 of 25000\n",
      "Cleaning review 16000 of 25000\n",
      "Cleaning review 17000 of 25000\n",
      "Cleaning review 18000 of 25000\n",
      "Cleaning review 19000 of 25000\n",
      "Cleaning review 20000 of 25000\n",
      "Cleaning review 21000 of 25000\n",
      "Cleaning review 22000 of 25000\n",
      "Cleaning review 23000 of 25000\n",
      "Cleaning review 24000 of 25000\n",
      "Cleaning review 25000 of 25000\n"
     ]
    }
   ],
   "source": [
    "# Get the number of reviews in the training set\n",
    "num_reviews = train[\"review\"].size\n",
    "\n",
    "# Initialize an empty list to hold the cleaned reviews\n",
    "cleaned_train_reviews = []\n",
    "\n",
    "# Iterate through each review in the training set\n",
    "for i in range(num_reviews):\n",
    "    if (i + 1) % 1000 == 0:\n",
    "        print(f\"Cleaning review {i + 1} of {num_reviews}\")\n",
    "    \n",
    "    # Clean the review and append it to the list\n",
    "    cleaned_review = clean_review_text(train[\"review\"][i])\n",
    "    cleaned_train_reviews.append(cleaned_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49b798a",
   "metadata": {},
   "source": [
    "Use bag-of-words approach to convert training reviews to numeric representation for machine learning. Build a vocabulary from all reviews, then create feature vectors with the count of each word in each review.\n",
    "\n",
    "To limit the size of the feature vectors, use the 5000 most frequent words. Use the `feature_extraction` module from `scikit-learn` to create bag-of-words features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e91d2db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training data features: (25000, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the CountVectorizer with parameters to limit the vocabulary size\n",
    "vectorizer = CountVectorizer(analyzer=\"word\", tokenizer=None, preprocessor=None, stop_words=None, max_features=5000)\n",
    "\n",
    "# Fit the vectorizer on the cleaned training reviews, learn the vocabulary, and transform the reviews into feature vectors\n",
    "train_data_features = vectorizer.fit_transform(cleaned_train_reviews)\n",
    "\n",
    "# Convert the resulting sparse matrix to a dense format\n",
    "train_data_features = train_data_features.toarray()\n",
    "\n",
    "print(\"Shape of the training data features:\", train_data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df498fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abandoned' 'abc' 'abilities' ... 'zombie' 'zombies' 'zone']\n"
     ]
    }
   ],
   "source": [
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911eedbf",
   "metadata": {},
   "source": [
    "Use a Random Forest classifier for supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f7a5408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the random forest classifier...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "print(\"Training the random forest classifier...\")\n",
    "\n",
    "# Initialize the Random Forest classifier with 100 trees\n",
    "forest = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Fit the forest to the training data features using the bag of words as features and the sentiment labels as the response variable\n",
    "forest = forest.fit(train_data_features, train[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f13dff",
   "metadata": {},
   "source": [
    "Run the trained Random Forest classifier on the test set and create a submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bfdb978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data shape: (25000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Read the test data\n",
    "test = pd.read_csv(\"../data/test_data.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "# Verify the test data shape\n",
    "print(\"Test data shape:\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c85a3df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning test reviews...\n",
      "Cleaning test review 1000 of 25000\n",
      "Cleaning test review 2000 of 25000\n",
      "Cleaning test review 3000 of 25000\n",
      "Cleaning test review 4000 of 25000\n",
      "Cleaning test review 5000 of 25000\n",
      "Cleaning test review 6000 of 25000\n",
      "Cleaning test review 7000 of 25000\n",
      "Cleaning test review 8000 of 25000\n",
      "Cleaning test review 9000 of 25000\n",
      "Cleaning test review 10000 of 25000\n",
      "Cleaning test review 11000 of 25000\n",
      "Cleaning test review 12000 of 25000\n",
      "Cleaning test review 13000 of 25000\n",
      "Cleaning test review 14000 of 25000\n",
      "Cleaning test review 15000 of 25000\n",
      "Cleaning test review 16000 of 25000\n",
      "Cleaning test review 17000 of 25000\n",
      "Cleaning test review 18000 of 25000\n",
      "Cleaning test review 19000 of 25000\n",
      "Cleaning test review 20000 of 25000\n",
      "Cleaning test review 21000 of 25000\n",
      "Cleaning test review 22000 of 25000\n",
      "Cleaning test review 23000 of 25000\n",
      "Cleaning test review 24000 of 25000\n",
      "Cleaning test review 25000 of 25000\n"
     ]
    }
   ],
   "source": [
    "# Clean and parse test reviews\n",
    "num_test_reviews = len(test[\"review\"])\n",
    "cleaned_test_reviews = []\n",
    "\n",
    "print(\"Cleaning test reviews...\")\n",
    "for i in range(num_test_reviews):\n",
    "    if (i + 1) % 1000 == 0:\n",
    "        print(f\"Cleaning test review {i + 1} of {num_test_reviews}\")\n",
    "    \n",
    "    cleaned_review = clean_review_text(test[\"review\"][i])\n",
    "    cleaned_test_reviews.append(cleaned_review)\n",
    "\n",
    "# Transform the cleaned test reviews into feature vectors\n",
    "test_data_features = vectorizer.transform(cleaned_test_reviews)\n",
    "test_data_features = test_data_features.toarray()\n",
    "\n",
    "# Use the trained Random Forest classifier to predict sentiment for the test set\n",
    "result = forest.predict(test_data_features)\n",
    "\n",
    "# Copy results to a DataFrame for submission\n",
    "output = pd.DataFrame(data={\"id\": test[\"id\"], \"sentiment\": result})\n",
    "\n",
    "# Write the DataFrame to a CSV file for submission\n",
    "output.to_csv(\"../data/tutorial_submission.csv\", index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8713bad3",
   "metadata": {},
   "source": [
    "## Using Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cc1cad",
   "metadata": {},
   "source": [
    "Word2vec is a neural network implementation that learns distributed representations for words. It does not need labels to create meaningful repretentations; if the network is given enough training data, it produces word vectors with intriguing characteristics. Words with similar meanings appear in clusters and clusters are spaced such that some word relationships, such as analogies, can be reproduced using vector math.\n",
    "\n",
    "First, can now use unlabeled training data in addition to labeled training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc9f537d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_train = pd.read_csv(\"../data/unlabeled_train_data.tsv\", header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c855bd6",
   "metadata": {},
   "source": [
    "To train Word2vec, it is better not to remove stopwords because the algorithm relies on the broader context of the sentence to produce high-quality word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "deb5904e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_tokens(review: str, remove_stopwords=False) -> list:\n",
    "    \"\"\"\n",
    "    Converts a review into a list of tokens, optionally removing stop words.\n",
    "\n",
    "    Parameters:\n",
    "        review (str): The raw review text to be tokenized.\n",
    "        remove_stopwords (bool): Whether to remove stop words from the tokens.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tokens from the review.\n",
    "    \"\"\"\n",
    "\n",
    "    # Parse the review using BeautifulSoup and remove HTML tags and markup\n",
    "    text = BeautifulSoup(review).get_text()\n",
    "\n",
    "    # Remove punctuation and numbers, convert to lower-case, and split into tokens\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    tokens = letters_only.lower().split()\n",
    "\n",
    "    # Optionally remove stop words\n",
    "    if remove_stopwords:\n",
    "        tokens = [token for token in tokens if token not in stops]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b5ea22",
   "metadata": {},
   "source": [
    "Word2vec expects single sentences, each one as a list of tokens. Need to decide how to split a paragraph into sentences. Use NLTK's punkt tokenizer for sentence splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a343dc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/rohanmistry/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "# Split the review into sentences and tokenize each sentence\n",
    "def review_to_sentences(review: str, remove_stopwords=False) -> list:\n",
    "    \"\"\"\n",
    "    Splits a review into sentences and tokenizes each sentence.\n",
    "\n",
    "    Parameters:\n",
    "        review (str): The raw review text to be split and tokenized.\n",
    "        tokenizer: The NLTK tokenizer for sentence splitting.\n",
    "        remove_stopwords (bool): Whether to remove stop words from the tokens.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of lists, where each inner list contains tokens from a sentence.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split the review into sentences\n",
    "    raw_sentences = sent_tokenize(review.strip())\n",
    "\n",
    "    # Tokenize each sentence\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # Check if the sentence is empty\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Tokenize the sentence\n",
    "            sentences = review_to_tokens(raw_sentence, remove_stopwords)\n",
    "            # Append the list of tokens to the result\n",
    "            yield sentences\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247dfda6",
   "metadata": {},
   "source": [
    "Apply `review_to_sentences` function to prepare data for input to Word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d64e330c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sentences from training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gy/bj1c9cj1435dy1g737vyx1yh0000gn/T/ipykernel_90192/249500784.py:14: MarkupResemblesLocatorWarning: The input passed in on this line looks more like a URL than HTML or XML.\n",
      "\n",
      "If you meant to use Beautiful Soup to parse the web page found at a certain URL, then something has gone wrong. You should use an Python package like 'requests' to fetch the content behind the URL. Once you have the content as a string, you can feed that string into Beautiful Soup.\n",
      "\n",
      "However, if you want to parse some data that happens to look like a URL, then nothing has gone wrong: you are using Beautiful Soup correctly, and this warning is spurious and can be filtered. To make this warning go away, run this code before calling the BeautifulSoup constructor:\n",
      "\n",
      "    from bs4 import MarkupResemblesLocatorWarning\n",
      "    import warnings\n",
      "\n",
      "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
      "    \n",
      "  text = BeautifulSoup(review).get_text()\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "print(\"Processing sentences from training data...\")\n",
    "for review in train[\"review\"]:\n",
    "    sentences += list(review_to_sentences(review))\n",
    "\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += list(review_to_sentences(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e09c8886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences processed: 796172\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of sentences processed:\", len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691df54f",
   "metadata": {},
   "source": [
    "## Training and Saving the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85374afe",
   "metadata": {},
   "source": [
    "There are many parameter choices that affect the runtime and quality of the final model produced, such as architecture, training algorithm, downsampling of frequent words, word vector dimensionality, context/window size, worker threads, and minimum word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a0e3be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 14:28:03,696 : INFO : collecting all words and their counts\n",
      "2025-08-05 14:28:03,697 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2025-08-05 14:28:03,742 : INFO : PROGRESS: at sentence #10000, processed 225664 words, keeping 17775 word types\n",
      "2025-08-05 14:28:03,768 : INFO : PROGRESS: at sentence #20000, processed 451738 words, keeping 24945 word types\n",
      "2025-08-05 14:28:03,789 : INFO : PROGRESS: at sentence #30000, processed 670858 words, keeping 30027 word types\n",
      "2025-08-05 14:28:03,811 : INFO : PROGRESS: at sentence #40000, processed 896840 words, keeping 34335 word types\n",
      "2025-08-05 14:28:03,834 : INFO : PROGRESS: at sentence #50000, processed 1116081 words, keeping 37751 word types\n",
      "2025-08-05 14:28:03,855 : INFO : PROGRESS: at sentence #60000, processed 1337543 words, keeping 40711 word types\n",
      "2025-08-05 14:28:03,876 : INFO : PROGRESS: at sentence #70000, processed 1560306 words, keeping 43311 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 14:28:03,903 : INFO : PROGRESS: at sentence #80000, processed 1779515 words, keeping 45707 word types\n",
      "2025-08-05 14:28:03,926 : INFO : PROGRESS: at sentence #90000, processed 2003713 words, keeping 48121 word types\n",
      "2025-08-05 14:28:03,943 : INFO : PROGRESS: at sentence #100000, processed 2225464 words, keeping 50190 word types\n",
      "2025-08-05 14:28:03,963 : INFO : PROGRESS: at sentence #110000, processed 2444322 words, keeping 52058 word types\n",
      "2025-08-05 14:28:03,982 : INFO : PROGRESS: at sentence #120000, processed 2666487 words, keeping 54098 word types\n",
      "2025-08-05 14:28:04,007 : INFO : PROGRESS: at sentence #130000, processed 2892314 words, keeping 55837 word types\n",
      "2025-08-05 14:28:04,026 : INFO : PROGRESS: at sentence #140000, processed 3104795 words, keeping 57324 word types\n",
      "2025-08-05 14:28:04,043 : INFO : PROGRESS: at sentence #150000, processed 3330431 words, keeping 59045 word types\n",
      "2025-08-05 14:28:04,065 : INFO : PROGRESS: at sentence #160000, processed 3552465 words, keeping 60581 word types\n",
      "2025-08-05 14:28:04,088 : INFO : PROGRESS: at sentence #170000, processed 3776047 words, keeping 62050 word types\n",
      "2025-08-05 14:28:04,112 : INFO : PROGRESS: at sentence #180000, processed 3996236 words, keeping 63483 word types\n",
      "2025-08-05 14:28:04,131 : INFO : PROGRESS: at sentence #190000, processed 4221287 words, keeping 64775 word types\n",
      "2025-08-05 14:28:04,155 : INFO : PROGRESS: at sentence #200000, processed 4445972 words, keeping 66070 word types\n",
      "2025-08-05 14:28:04,174 : INFO : PROGRESS: at sentence #210000, processed 4666510 words, keeping 67367 word types\n",
      "2025-08-05 14:28:04,199 : INFO : PROGRESS: at sentence #220000, processed 4892036 words, keeping 68686 word types\n",
      "2025-08-05 14:28:04,217 : INFO : PROGRESS: at sentence #230000, processed 5113881 words, keeping 69935 word types\n",
      "2025-08-05 14:28:04,242 : INFO : PROGRESS: at sentence #240000, processed 5340847 words, keeping 71144 word types\n",
      "2025-08-05 14:28:04,260 : INFO : PROGRESS: at sentence #250000, processed 5555463 words, keeping 72333 word types\n",
      "2025-08-05 14:28:04,284 : INFO : PROGRESS: at sentence #260000, processed 5775304 words, keeping 73466 word types\n",
      "2025-08-05 14:28:04,304 : INFO : PROGRESS: at sentence #270000, processed 5995572 words, keeping 74740 word types\n",
      "2025-08-05 14:28:04,330 : INFO : PROGRESS: at sentence #280000, processed 6220911 words, keeping 76318 word types\n",
      "2025-08-05 14:28:04,348 : INFO : PROGRESS: at sentence #290000, processed 6443523 words, keeping 77787 word types\n",
      "2025-08-05 14:28:04,367 : INFO : PROGRESS: at sentence #300000, processed 6668258 words, keeping 79142 word types\n",
      "2025-08-05 14:28:04,386 : INFO : PROGRESS: at sentence #310000, processed 6892662 words, keeping 80431 word types\n",
      "2025-08-05 14:28:04,405 : INFO : PROGRESS: at sentence #320000, processed 7118969 words, keeping 81794 word types\n",
      "2025-08-05 14:28:04,424 : INFO : PROGRESS: at sentence #330000, processed 7340486 words, keeping 83006 word types\n",
      "2025-08-05 14:28:04,447 : INFO : PROGRESS: at sentence #340000, processed 7569986 words, keeping 84252 word types\n",
      "2025-08-05 14:28:04,465 : INFO : PROGRESS: at sentence #350000, processed 7792927 words, keeping 85407 word types\n",
      "2025-08-05 14:28:04,490 : INFO : PROGRESS: at sentence #360000, processed 8012526 words, keeping 86567 word types\n",
      "2025-08-05 14:28:04,520 : INFO : PROGRESS: at sentence #370000, processed 8239772 words, keeping 87663 word types\n",
      "2025-08-05 14:28:04,542 : INFO : PROGRESS: at sentence #380000, processed 8465827 words, keeping 88849 word types\n",
      "2025-08-05 14:28:04,565 : INFO : PROGRESS: at sentence #390000, processed 8694607 words, keeping 89883 word types\n",
      "2025-08-05 14:28:04,591 : INFO : PROGRESS: at sentence #400000, processed 8917820 words, keeping 90882 word types\n",
      "2025-08-05 14:28:04,613 : INFO : PROGRESS: at sentence #410000, processed 9138504 words, keeping 91859 word types\n",
      "2025-08-05 14:28:04,638 : INFO : PROGRESS: at sentence #420000, processed 9358474 words, keeping 92880 word types\n",
      "2025-08-05 14:28:04,664 : INFO : PROGRESS: at sentence #430000, processed 9586958 words, keeping 93909 word types\n",
      "2025-08-05 14:28:04,684 : INFO : PROGRESS: at sentence #440000, processed 9812576 words, keeping 94853 word types\n",
      "2025-08-05 14:28:04,712 : INFO : PROGRESS: at sentence #450000, processed 10036719 words, keeping 95995 word types\n",
      "2025-08-05 14:28:04,735 : INFO : PROGRESS: at sentence #460000, processed 10269931 words, keeping 97064 word types\n",
      "2025-08-05 14:28:04,759 : INFO : PROGRESS: at sentence #470000, processed 10496262 words, keeping 97885 word types\n",
      "2025-08-05 14:28:04,783 : INFO : PROGRESS: at sentence #480000, processed 10717170 words, keeping 98809 word types\n",
      "2025-08-05 14:28:04,807 : INFO : PROGRESS: at sentence #490000, processed 10943335 words, keeping 99835 word types\n",
      "2025-08-05 14:28:04,829 : INFO : PROGRESS: at sentence #500000, processed 11165141 words, keeping 100726 word types\n",
      "2025-08-05 14:28:04,854 : INFO : PROGRESS: at sentence #510000, processed 11390498 words, keeping 101672 word types\n",
      "2025-08-05 14:28:04,874 : INFO : PROGRESS: at sentence #520000, processed 11613511 words, keeping 102557 word types\n",
      "2025-08-05 14:28:04,904 : INFO : PROGRESS: at sentence #530000, processed 11838774 words, keeping 103374 word types\n",
      "2025-08-05 14:28:04,943 : INFO : PROGRESS: at sentence #540000, processed 12062185 words, keeping 104231 word types\n",
      "2025-08-05 14:28:04,965 : INFO : PROGRESS: at sentence #550000, processed 12286959 words, keeping 105098 word types\n",
      "2025-08-05 14:28:04,991 : INFO : PROGRESS: at sentence #560000, processed 12509034 words, keeping 105971 word types\n",
      "2025-08-05 14:28:05,016 : INFO : PROGRESS: at sentence #570000, processed 12736827 words, keeping 106757 word types\n",
      "2025-08-05 14:28:05,036 : INFO : PROGRESS: at sentence #580000, processed 12958427 words, keeping 107611 word types\n",
      "2025-08-05 14:28:05,066 : INFO : PROGRESS: at sentence #590000, processed 13184324 words, keeping 108469 word types\n",
      "2025-08-05 14:28:05,085 : INFO : PROGRESS: at sentence #600000, processed 13406550 words, keeping 109190 word types\n",
      "2025-08-05 14:28:05,110 : INFO : PROGRESS: at sentence #610000, processed 13628197 words, keeping 110056 word types\n",
      "2025-08-05 14:28:05,132 : INFO : PROGRESS: at sentence #620000, processed 13852587 words, keeping 110806 word types\n",
      "2025-08-05 14:28:05,156 : INFO : PROGRESS: at sentence #630000, processed 14075900 words, keeping 111574 word types\n",
      "2025-08-05 14:28:05,174 : INFO : PROGRESS: at sentence #640000, processed 14298045 words, keeping 112387 word types\n",
      "2025-08-05 14:28:05,197 : INFO : PROGRESS: at sentence #650000, processed 14522873 words, keeping 113152 word types\n",
      "2025-08-05 14:28:05,216 : INFO : PROGRESS: at sentence #660000, processed 14745444 words, keeping 113891 word types\n",
      "2025-08-05 14:28:05,245 : INFO : PROGRESS: at sentence #670000, processed 14970568 words, keeping 114614 word types\n",
      "2025-08-05 14:28:05,267 : INFO : PROGRESS: at sentence #680000, processed 15194624 words, keeping 115332 word types\n",
      "2025-08-05 14:28:05,290 : INFO : PROGRESS: at sentence #690000, processed 15416772 words, keeping 116100 word types\n",
      "2025-08-05 14:28:05,314 : INFO : PROGRESS: at sentence #700000, processed 15645694 words, keeping 116903 word types\n",
      "2025-08-05 14:28:05,336 : INFO : PROGRESS: at sentence #710000, processed 15865814 words, keeping 117542 word types\n",
      "2025-08-05 14:28:05,363 : INFO : PROGRESS: at sentence #720000, processed 16093341 words, keeping 118184 word types\n",
      "2025-08-05 14:28:05,384 : INFO : PROGRESS: at sentence #730000, processed 16316786 words, keeping 118913 word types\n",
      "2025-08-05 14:28:05,411 : INFO : PROGRESS: at sentence #740000, processed 16539145 words, keeping 119619 word types\n",
      "2025-08-05 14:28:05,430 : INFO : PROGRESS: at sentence #750000, processed 16758550 words, keeping 120265 word types\n",
      "2025-08-05 14:28:05,450 : INFO : PROGRESS: at sentence #760000, processed 16977109 words, keeping 120889 word types\n",
      "2025-08-05 14:28:05,476 : INFO : PROGRESS: at sentence #770000, processed 17203257 words, keeping 121657 word types\n",
      "2025-08-05 14:28:05,500 : INFO : PROGRESS: at sentence #780000, processed 17432842 words, keeping 122359 word types\n",
      "2025-08-05 14:28:05,521 : INFO : PROGRESS: at sentence #790000, processed 17660149 words, keeping 123034 word types\n",
      "2025-08-05 14:28:05,538 : INFO : collected 123505 word types from a corpus of 17798268 raw words and 796172 sentences\n",
      "2025-08-05 14:28:05,538 : INFO : Creating a fresh vocabulary\n",
      "2025-08-05 14:28:05,577 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=40 retains 16490 unique words (13.35% of original 123505, drops 107015)', 'datetime': '2025-08-05T14:28:05.577823', 'gensim': '4.3.3', 'python': '3.10.13 (main, Jul 28 2025, 15:33:42) [Clang 16.0.0 (clang-1600.0.26.6)]', 'platform': 'macOS-15.2-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2025-08-05 14:28:05,578 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=40 leaves 17239122 word corpus (96.86% of original 17798268, drops 559146)', 'datetime': '2025-08-05T14:28:05.578248', 'gensim': '4.3.3', 'python': '3.10.13 (main, Jul 28 2025, 15:33:42) [Clang 16.0.0 (clang-1600.0.26.6)]', 'platform': 'macOS-15.2-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2025-08-05 14:28:05,605 : INFO : deleting the raw counts dictionary of 123505 items\n",
      "2025-08-05 14:28:05,607 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2025-08-05 14:28:05,607 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 12749795.99803372 word corpus (74.0%% of prior 17239122)', 'datetime': '2025-08-05T14:28:05.607466', 'gensim': '4.3.3', 'python': '3.10.13 (main, Jul 28 2025, 15:33:42) [Clang 16.0.0 (clang-1600.0.26.6)]', 'platform': 'macOS-15.2-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2025-08-05 14:28:05,647 : INFO : estimated required memory for 16490 words and 300 dimensions: 47821000 bytes\n",
      "2025-08-05 14:28:05,648 : INFO : resetting layer weights\n",
      "2025-08-05 14:28:05,661 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-08-05T14:28:05.661492', 'gensim': '4.3.3', 'python': '3.10.13 (main, Jul 28 2025, 15:33:42) [Clang 16.0.0 (clang-1600.0.26.6)]', 'platform': 'macOS-15.2-arm64-arm-64bit', 'event': 'build_vocab'}\n",
      "2025-08-05 14:28:05,661 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 16490 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2025-08-05T14:28:05.661912', 'gensim': '4.3.3', 'python': '3.10.13 (main, Jul 28 2025, 15:33:42) [Clang 16.0.0 (clang-1600.0.26.6)]', 'platform': 'macOS-15.2-arm64-arm-64bit', 'event': 'train'}\n",
      "2025-08-05 14:28:06,666 : INFO : EPOCH 0 - PROGRESS: at 16.06% examples, 2034502 words/s, in_qsize 7, out_qsize 0\n",
      "2025-08-05 14:28:07,666 : INFO : EPOCH 0 - PROGRESS: at 33.66% examples, 2128375 words/s, in_qsize 7, out_qsize 0\n",
      "2025-08-05 14:28:08,669 : INFO : EPOCH 0 - PROGRESS: at 51.59% examples, 2182136 words/s, in_qsize 7, out_qsize 0\n",
      "2025-08-05 14:28:09,671 : INFO : EPOCH 0 - PROGRESS: at 69.54% examples, 2211306 words/s, in_qsize 7, out_qsize 0\n",
      "2025-08-05 14:28:10,676 : INFO : EPOCH 0 - PROGRESS: at 87.59% examples, 2227453 words/s, in_qsize 7, out_qsize 0\n",
      "2025-08-05 14:28:11,421 : INFO : EPOCH 0: training on 17798268 raw words (12752062 effective words) took 5.8s, 2214919 effective words/s\n",
      "2025-08-05 14:28:12,431 : INFO : EPOCH 1 - PROGRESS: at 18.13% examples, 2277483 words/s, in_qsize 7, out_qsize 0\n",
      "2025-08-05 14:28:13,439 : INFO : EPOCH 1 - PROGRESS: at 36.40% examples, 2287075 words/s, in_qsize 7, out_qsize 0\n",
      "2025-08-05 14:28:14,444 : INFO : EPOCH 1 - PROGRESS: at 54.26% examples, 2284231 words/s, in_qsize 7, out_qsize 0\n",
      "2025-08-05 14:28:15,444 : INFO : EPOCH 1 - PROGRESS: at 72.10% examples, 2285314 words/s, in_qsize 7, out_qsize 0\n",
      "2025-08-05 14:28:16,446 : INFO : EPOCH 1 - PROGRESS: at 90.13% examples, 2287501 words/s, in_qsize 7, out_qsize 0\n",
      "2025-08-05 14:28:16,991 : INFO : EPOCH 1: training on 17798268 raw words (12748077 effective words) took 5.6s, 2289776 effective words/s\n",
      "2025-08-05 14:28:17,994 : INFO : EPOCH 2 - PROGRESS: at 17.41% examples, 2201877 words/s, in_qsize 7, out_qsize 0\n",
      "2025-08-05 14:28:19,003 : INFO : EPOCH 2 - PROGRESS: at 34.99% examples, 2205525 words/s, in_qsize 7, out_qsize 0\n",
      "2025-08-05 14:28:20,003 : INFO : EPOCH 2 - PROGRESS: at 52.32% examples, 2209681 words/s, in_qsize 8, out_qsize 0\n",
      "2025-08-05 14:28:21,003 : INFO : EPOCH 2 - PROGRESS: at 69.37% examples, 2204200 words/s, in_qsize 7, out_qsize 0\n",
      "2025-08-05 14:28:22,006 : INFO : EPOCH 2 - PROGRESS: at 86.70% examples, 2203822 words/s, in_qsize 7, out_qsize 0\n",
      "2025-08-05 14:28:22,773 : INFO : EPOCH 2: training on 17798268 raw words (12750085 effective words) took 5.8s, 2205863 effective words/s\n",
      "2025-08-05 14:28:23,781 : INFO : EPOCH 3 - PROGRESS: at 17.41% examples, 2191405 words/s, in_qsize 7, out_qsize 0\n",
      "2025-08-05 14:28:24,785 : INFO : EPOCH 3 - PROGRESS: at 34.88% examples, 2198421 words/s, in_qsize 7, out_qsize 0\n",
      "2025-08-05 14:28:25,789 : INFO : EPOCH 3 - PROGRESS: at 52.10% examples, 2196743 words/s, in_qsize 7, out_qsize 0\n",
      "2025-08-05 14:28:26,799 : INFO : EPOCH 3 - PROGRESS: at 69.37% examples, 2196680 words/s, in_qsize 7, out_qsize 0\n",
      "2025-08-05 14:28:27,808 : INFO : EPOCH 3 - PROGRESS: at 86.81% examples, 2198086 words/s, in_qsize 7, out_qsize 0\n",
      "2025-08-05 14:28:28,577 : INFO : EPOCH 3: training on 17798268 raw words (12749089 effective words) took 5.8s, 2197422 effective words/s\n",
      "2025-08-05 14:28:29,582 : INFO : EPOCH 4 - PROGRESS: at 17.41% examples, 2198971 words/s, in_qsize 7, out_qsize 0\n",
      "2025-08-05 14:28:30,591 : INFO : EPOCH 4 - PROGRESS: at 34.99% examples, 2203807 words/s, in_qsize 7, out_qsize 0\n",
      "2025-08-05 14:28:31,592 : INFO : EPOCH 4 - PROGRESS: at 51.65% examples, 2179316 words/s, in_qsize 7, out_qsize 0\n",
      "2025-08-05 14:28:32,603 : INFO : EPOCH 4 - PROGRESS: at 68.99% examples, 2184260 words/s, in_qsize 7, out_qsize 0\n",
      "2025-08-05 14:28:33,612 : INFO : EPOCH 4 - PROGRESS: at 86.59% examples, 2192719 words/s, in_qsize 7, out_qsize 0\n",
      "2025-08-05 14:28:34,392 : INFO : EPOCH 4: training on 17798268 raw words (12749447 effective words) took 5.8s, 2193468 effective words/s\n",
      "2025-08-05 14:28:34,393 : INFO : Word2Vec lifecycle event {'msg': 'training on 88991340 raw words (63748760 effective words) took 28.7s, 2218793 effective words/s', 'datetime': '2025-08-05T14:28:34.393366', 'gensim': '4.3.3', 'python': '3.10.13 (main, Jul 28 2025, 15:33:42) [Clang 16.0.0 (clang-1600.0.26.6)]', 'platform': 'macOS-15.2-arm64-arm-64bit', 'event': 'train'}\n",
      "2025-08-05 14:28:34,393 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=16490, vector_size=300, alpha=0.025>', 'datetime': '2025-08-05T14:28:34.393622', 'gensim': '4.3.3', 'python': '3.10.13 (main, Jul 28 2025, 15:33:42) [Clang 16.0.0 (clang-1600.0.26.6)]', 'platform': 'macOS-15.2-arm64-arm-64bit', 'event': 'created'}\n",
      "/var/folders/gy/bj1c9cj1435dy1g737vyx1yh0000gn/T/ipykernel_90192/1903003158.py:15: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  model.init_sims(replace=True)\n",
      "2025-08-05 14:28:34,398 : WARNING : destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n",
      "2025-08-05 14:28:34,399 : INFO : Word2Vec lifecycle event {'fname_or_handle': '../models/word2vec_model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2025-08-05T14:28:34.399424', 'gensim': '4.3.3', 'python': '3.10.13 (main, Jul 28 2025, 15:33:42) [Clang 16.0.0 (clang-1600.0.26.6)]', 'platform': 'macOS-15.2-arm64-arm-64bit', 'event': 'saving'}\n",
      "2025-08-05 14:28:34,400 : INFO : not storing attribute cum_table\n",
      "2025-08-05 14:28:34,418 : INFO : saved ../models/word2vec_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec model saved to ../models/word2vec_model\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO)\n",
    "\n",
    "# Set values for Word2vec parameters\n",
    "num_features = 300    # Word vector dimensionality\n",
    "min_word_count = 40   # Minimum word count to consider a word in the vocabulary\n",
    "num_workers = 4       # Number of worker threads to train the model\n",
    "context = 10          # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model\n",
    "from gensim.models import Word2Vec\n",
    "print(\"Training Word2Vec model...\")\n",
    "model = Word2Vec(sentences, workers=num_workers, vector_size=num_features, min_count=min_word_count, window=context, sample=downsampling)\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# Save the model to a file\n",
    "model_file = \"../models/word2vec_model\"\n",
    "model.save(model_file)\n",
    "print(f\"Word2Vec model saved to {model_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1577eb45",
   "metadata": {},
   "source": [
    "## Exploring the Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35c08aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which doesn't match between 'man', 'woman', 'child', and 'kitchen'?  kitchen\n",
      "Which doesn't match between 'france', 'england', 'germany', and 'berlin'?  berlin\n",
      "Which doesn't match between 'paris', 'berlin', 'london', and 'austria'?  paris\n",
      "Most similar to 'man':  [('central', 0.6256492733955383), ('secondary', 0.5544471740722656), ('primary', 0.551468014717102), ('development', 0.49399852752685547), ('peripheral', 0.49228551983833313), ('undeveloped', 0.4839439392089844), ('titular', 0.4774162471294403), ('major', 0.4698925018310547), ('unlikeable', 0.4575895667076111), ('biggest', 0.4439127445220947)]\n",
      "Most similar to 'queen':  [('princess', 0.6462267637252808), ('victoria', 0.6021219491958618), ('bride', 0.5906339883804321), ('goddess', 0.5817415714263916), ('maid', 0.5766915678977966), ('belle', 0.5599071383476257), ('prince', 0.5592232346534729), ('showgirl', 0.558584451675415), ('fatale', 0.558175802230835), ('maria', 0.5564477443695068)]\n",
      "Most similar to 'awful':  [('terrible', 0.7692235112190247), ('horrible', 0.7320711612701416), ('atrocious', 0.7264685034751892), ('abysmal', 0.7023158073425293), ('dreadful', 0.7015078067779541), ('horrendous', 0.6812111139297485), ('appalling', 0.6655059456825256), ('horrid', 0.6462688446044922), ('amateurish', 0.6023362874984741), ('bad', 0.5926852226257324)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Which doesn't match between 'man', 'woman', 'child', and 'kitchen'? \", model.wv.doesnt_match(\"man woman child kitchen\".split()))\n",
    "print(\"Which doesn't match between 'france', 'england', 'germany', and 'berlin'? \", model.wv.doesnt_match(\"france england germany berlin\".split()))\n",
    "print(\"Which doesn't match between 'paris', 'berlin', 'london', and 'austria'? \", model.wv.doesnt_match(\"paris berlin london austria\".split()))\n",
    "print(\"Most similar to 'man': \", model.wv.most_similar(\"main\"))\n",
    "print(\"Most similar to 'queen': \", model.wv.most_similar(\"queen\"))\n",
    "print(\"Most similar to 'awful': \", model.wv.most_similar(\"awful\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3089ba92",
   "metadata": {},
   "source": [
    "## Numeric Representation of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82349b23",
   "metadata": {},
   "source": [
    "The trained Word2vec model consists of a feature vector for each word in the vocabulary, stored in a `numpy` array called \"vectors\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "583153f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the word vectors (syn0): (16490, 300)\n"
     ]
    }
   ],
   "source": [
    "type(model.wv.vectors)\n",
    "print(\"Shape of the word vectors (syn0):\", model.wv.vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df535e1c",
   "metadata": {},
   "source": [
    "The number of each row is the number of words in the model's vocabulary and the number corresponds to the size of the feature vector. Setting the minimum word count to 40 means that the total vocabulary consists of approximately 16,490 words with 300 features each. Individual word vectors (a 1 x 300 `numpy` array) can be accessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f761ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.01115685e-02  1.29889371e-02 -2.08567455e-02  8.65000933e-02\n",
      "  2.41670031e-02 -4.74770330e-02  1.91409495e-02  9.70198661e-02\n",
      "  1.05710821e-02 -5.11368178e-02 -2.05972921e-02  4.30359952e-02\n",
      " -2.48122811e-02  8.83926451e-02 -4.75177281e-02 -2.69599017e-02\n",
      "  1.27959223e-02 -1.27007991e-01  6.86568115e-03 -3.40595134e-02\n",
      " -1.24566734e-03 -2.57888157e-02  3.92788351e-02  9.20327082e-02\n",
      "  7.03837350e-02 -1.54855298e-02 -1.91287249e-02  2.30791830e-02\n",
      "  6.86938036e-03  4.37342236e-03  4.70337681e-02 -3.90385799e-02\n",
      " -1.34434234e-02 -3.38446535e-02  1.04363887e-02  6.28910437e-02\n",
      "  5.69956750e-02 -1.17500491e-01 -4.45946008e-02 -3.11273336e-02\n",
      " -2.11478807e-02  7.73067819e-03  9.06712934e-02 -1.16943354e-02\n",
      " -1.15378015e-01  1.76322833e-03 -4.87123895e-03 -9.07074846e-03\n",
      " -2.58970149e-02  1.85833052e-02  6.70485795e-02  3.26483399e-02\n",
      " -3.44926640e-02 -5.82870608e-03 -2.64347177e-02  4.23103645e-02\n",
      " -1.08057745e-02 -3.82160163e-03 -1.67368837e-02 -9.27755088e-02\n",
      " -7.69823045e-02  3.73006016e-02 -1.12746507e-02  4.86920075e-03\n",
      " -4.07551080e-02  3.85841578e-02 -1.00033559e-01 -5.01832552e-03\n",
      "  4.23040837e-02 -8.46057907e-02 -7.22511336e-02  5.96871339e-02\n",
      "  3.69872712e-02 -1.10744592e-02 -3.59513871e-02 -8.54225480e-04\n",
      " -6.84495717e-02  1.03429392e-01  8.03176127e-03 -1.01529947e-02\n",
      "  3.01876501e-03 -1.94540771e-03 -2.31530685e-02  1.51168242e-01\n",
      "  4.85849392e-04  5.17257154e-02 -4.40919437e-02  2.98596658e-02\n",
      "  7.51154497e-02 -4.17805947e-02 -9.53567028e-02  6.78131217e-03\n",
      "  7.60429830e-04  3.02906241e-02  6.86868429e-02 -5.90489479e-03\n",
      "  9.47619900e-02  1.17793214e-02 -1.38827339e-01  4.12508100e-02\n",
      "  5.22233034e-03  4.75646034e-02  1.61999781e-02 -5.79288453e-02\n",
      "  6.73349872e-02  4.11656946e-02  4.17581648e-02 -4.45020124e-02\n",
      "  5.64305559e-02 -1.92746669e-02  2.62614656e-02  1.15117682e-02\n",
      "  2.38059275e-02  1.12345666e-01  4.28614095e-02  4.71060053e-02\n",
      "  3.36968824e-02  1.23112798e-01 -3.30551378e-02  3.67993191e-02\n",
      "  8.31520651e-03  4.97021973e-02 -5.29998504e-02  1.20658353e-02\n",
      "  1.73794013e-02  4.58828025e-02  5.21958694e-02 -3.58084007e-03\n",
      "  2.48635300e-02  2.76317983e-03 -6.92218244e-02  1.27581856e-03\n",
      " -5.08545898e-02 -1.08628795e-02  2.88868714e-02 -4.04350273e-02\n",
      "  2.46325284e-02  4.60443161e-02 -3.01155243e-02  6.17517792e-02\n",
      " -7.86044151e-02 -8.84505734e-02 -1.75542887e-02  2.92081889e-02\n",
      "  3.96030992e-02 -2.17732675e-02 -5.78195043e-02 -5.31932041e-02\n",
      "  1.16119206e-01 -7.79274676e-04 -1.60747413e-02 -1.28461912e-01\n",
      " -1.44734904e-01  1.79121010e-02 -1.67749880e-04 -1.43413991e-02\n",
      " -3.65424156e-02 -3.40310708e-02 -1.01214983e-01  6.02444150e-02\n",
      "  1.24785816e-02 -7.16811744e-03 -5.05070984e-02  8.56966302e-02\n",
      " -1.06337860e-01 -2.20588539e-02  1.15184382e-01 -5.23787700e-02\n",
      "  4.94337864e-02  9.86379758e-02  1.66062061e-02  6.80809543e-02\n",
      " -2.79456582e-02  5.20940870e-02 -9.15766954e-02 -6.41250089e-02\n",
      "  7.18909875e-02  1.46355582e-02 -5.39258830e-02  4.76572551e-02\n",
      "  3.53119783e-02  8.02596584e-02  6.99243769e-02 -7.96839818e-02\n",
      " -2.70464793e-02  2.91978642e-02  3.32390256e-02 -1.22542260e-02\n",
      "  2.62481309e-02  2.91733090e-02  8.57541151e-03  4.96609462e-03\n",
      "  5.38956597e-02  3.45206484e-02 -4.77709882e-02 -1.86336250e-03\n",
      " -2.79683862e-02 -2.58759651e-02  2.09749248e-02 -3.18820328e-02\n",
      " -5.85914217e-02  1.03317782e-01  4.81922179e-02 -5.19119874e-02\n",
      " -2.20534690e-02  3.53184603e-02  5.39223552e-02  1.83212787e-01\n",
      " -2.12144013e-02  8.07222445e-03  7.78237954e-02 -6.66502267e-02\n",
      "  8.81442893e-03 -6.07927069e-02  8.24335311e-03 -8.92215315e-03\n",
      " -1.35676973e-02 -1.84618726e-01  5.45894578e-02 -1.79231633e-02\n",
      "  1.30375668e-01  5.50076105e-02  3.86772230e-02  7.85560459e-02\n",
      " -1.64052192e-02  4.75706346e-03 -6.71513826e-02 -4.52117510e-02\n",
      " -7.76143596e-02 -5.35323061e-02 -5.81163075e-03 -2.03665104e-02\n",
      " -6.11711293e-02 -1.16762351e-02  1.00205792e-02  4.53493223e-02\n",
      " -8.08620974e-02  3.23020145e-02 -2.58535352e-02 -8.69524479e-02\n",
      "  4.35242802e-02  2.06544120e-02 -8.49063769e-02 -2.64090113e-02\n",
      " -1.61185134e-02  4.12110379e-03  7.03428760e-02  2.63784118e-02\n",
      " -6.69454262e-02  2.63409223e-02  8.90892744e-03  2.60458812e-02\n",
      "  8.50620195e-02 -2.44860295e-02  3.90169863e-03 -9.94531214e-02\n",
      "  4.52224389e-02  5.64854778e-03 -6.85229748e-02 -1.39583796e-02\n",
      " -1.11105451e-02 -3.74393836e-02 -5.24655581e-02 -3.85673009e-02\n",
      " -4.02086377e-02  1.20598199e-02  6.54659346e-02  3.52374054e-02\n",
      " -4.99787740e-02 -4.30058427e-02 -9.18973237e-02  7.63795301e-02\n",
      " -3.95136997e-02 -4.25280333e-02  1.51832357e-01  9.05786734e-03\n",
      "  1.63552374e-01  6.03579693e-02 -9.24171582e-02  1.01858529e-03\n",
      " -3.61150950e-02 -9.22162607e-02 -1.99089255e-02 -9.50334966e-03\n",
      "  1.09140545e-01 -1.14282742e-01 -1.12615414e-01  1.16635568e-01\n",
      "  2.72749104e-02  3.81402597e-02 -3.74308266e-02  6.31922930e-02\n",
      "  1.28279924e-01  9.29661002e-03  1.07712045e-01  1.07431643e-01\n",
      " -2.19949950e-02  1.80319231e-02 -3.32705257e-03 -7.59571791e-03]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv[\"flower\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40ba8ed",
   "metadata": {},
   "source": [
    "## From Words to Paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3341c2",
   "metadata": {},
   "source": [
    "### Vector Averaging\n",
    "\n",
    "One challenge is variable-length reviews. Need to take individual word vectors and transform them into a feature set that is the same length for every review. Since each word is a vector in 300-dimensional space, use vector operations to combine the words in each review. One method is to simply average the word vectors in a given review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f5a5cef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def make_feature_vector(words: list, model: Word2Vec, num_features: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculates the average feature vector for a list of words.\n",
    "\n",
    "    Parameters:\n",
    "        words (list): A list of words to be averaged.\n",
    "        model (Word2Vec): The trained Word2Vec model.\n",
    "        num_features (int): The dimensionality of the feature vectors.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A numpy array representing the average feature vector for the words.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize an empty feature vector of zeros\n",
    "    feature_vector = np.zeros((num_features,), dtype=\"float32\")\n",
    "    num_words = 0\n",
    "\n",
    "    # Index2word is a list that contains the names of the words in the model's vocabulary\n",
    "    index2word_set = set(model.wv.index_to_key)\n",
    "\n",
    "    # Iterate through each word in the review and add its feature vector to the total if it is in the model's vocabulary\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            num_words = num_words + 1.\n",
    "            feature_vector = np.add(feature_vector, model.wv[word])\n",
    "\n",
    "    # Divide the result by the number of words to get the average\n",
    "    if num_words > 0:\n",
    "        feature_vector = np.divide(feature_vector, num_words)\n",
    "\n",
    "    return feature_vector\n",
    "\n",
    "\n",
    "def get_average_feature_vectors(reviews, model, num_features):\n",
    "    \"\"\"\n",
    "    Calculates the average feature vector for each review.\n",
    "\n",
    "    Parameters:\n",
    "        reviews (list): A list of reviews, where each review is a list of words.\n",
    "        model (Word2Vec): The trained Word2Vec model.\n",
    "        num_features (int): The dimensionality of the feature vectors.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: A 2D numpy array where each row corresponds to the average feature vector of a review.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize a counter\n",
    "    counter = 0\n",
    "\n",
    "    # Initialize an empty array to hold the feature vectors\n",
    "    review_feature_vectors = np.zeros((len(reviews), num_features), dtype=\"float32\")\n",
    "\n",
    "    # Iterate through the reviews\n",
    "    for review in reviews:\n",
    "       # Print a status message every 1000th review\n",
    "       if counter % 1000 == 0.:\n",
    "           print(\"Review %d of %d\" % (counter, len(reviews)))\n",
    "\n",
    "       # Make average feature vector for the review\n",
    "       review_feature_vectors[counter] = make_feature_vector(review, model, num_features)\n",
    "       counter = counter + 1\n",
    "\n",
    "    return review_feature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8a4241b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating average feature vectors for training reviews...\n",
      "Review 0 of 25000\n",
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n",
      "Calculating average feature vectors for test reviews...\n",
      "Review 0 of 25000\n",
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average feature vectors for the training reviews\n",
    "print(\"Calculating average feature vectors for training reviews...\")\n",
    "clean_train_reviews = []\n",
    "for review in train[\"review\"]:\n",
    "    clean_train_reviews.append(review_to_tokens(review, remove_stopwords=True))\n",
    "\n",
    "train_vectors = get_average_feature_vectors(clean_train_reviews, model, num_features)\n",
    "\n",
    "# Calculate the average feature vectors for the test reviews\n",
    "print(\"Calculating average feature vectors for test reviews...\")\n",
    "clean_test_reviews = []\n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append(review_to_tokens(review, remove_stopwords=True))\n",
    "    \n",
    "test_vectors = get_average_feature_vectors(clean_test_reviews, model, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc00fe5a",
   "metadata": {},
   "source": [
    "Use the average paragraph vectors to train a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "25223699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the Random Forest classifier to the labeled training data...\n",
      "Predicting sentiment for the test data...\n"
     ]
    }
   ],
   "source": [
    "# Fit a Random Forest classifier to the training data\n",
    "print(\"Fitting the Random Forest classifier to the labeled training data...\")\n",
    "forest = RandomForestClassifier(n_estimators=100)\n",
    "forest = forest.fit(train_vectors, train[\"sentiment\"])\n",
    "\n",
    "# Test the model on the test data\n",
    "print(\"Predicting sentiment for the test data...\")\n",
    "result = forest.predict(test_vectors)\n",
    "\n",
    "# Copy results to a DataFrame for submission\n",
    "output = pd.DataFrame(data={\"id\": test[\"id\"], \"sentiment\": result})\n",
    "\n",
    "# Write the DataFrame to a CSV file for submission\n",
    "output.to_csv(\"../data/word2vec_average_vectors.csv\", index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c6e952",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "Another approach is to exploit the similarity of words within a cluster. Grouping vectors in this way is known as vector quantization. To accomplish this, find the centers of the word clusters by using a clustering algorithm such as K-means.\n",
    "\n",
    "In K-means, the one parameter to set is `k`, the number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "22a770d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to cluster the words into 3298 clusters: 38.93 seconds\n",
      "\n",
      "Cluster 0:\n",
      "marjorie\n",
      "\n",
      "Cluster 1:\n",
      "es heaps gigli lingo glitter\n",
      "\n",
      "Cluster 2:\n",
      "underwhelming incongruous recognisable ridicules patchy\n",
      "\n",
      "Cluster 3:\n",
      "head mouth skin arms teeth legs chair neck arm leg throat chest fingers bite skull blast penis limbs toe crotch eyeballs slit\n",
      "\n",
      "Cluster 4:\n",
      "repressed controlling fierce fearful persistent unspoken childlike liberated promiscuous deviant virtuous cautious carnal\n",
      "\n",
      "Cluster 5:\n",
      "cynicism craziness discomfort naivety\n",
      "\n",
      "Cluster 6:\n",
      "homes towns villages artifacts\n",
      "\n",
      "Cluster 7:\n",
      "ringo starr untouchables yoko zappa\n",
      "\n",
      "Cluster 8:\n",
      "commando cobra\n",
      "\n",
      "Cluster 9:\n",
      "clumsy stilted plotting limp snappy overwrought sparse clunky disconnected fisted haphazard leaden jumpy pointlessly uninvolving listless sketchy unintelligible audible stagy laboured\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Set k to be 1/5th of the vocabulary size\n",
    "num_clusters = model.wv.vectors.shape[0] // 5\n",
    "\n",
    "# Initialize a k-means clustering model and use it to extract the centroids\n",
    "kmeans_clustering = KMeans(n_clusters=num_clusters)\n",
    "idx = kmeans_clustering.fit_predict(model.wv.vectors)\n",
    "\n",
    "# Get end time and print how long process took\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(f\"Time taken to cluster the words into {num_clusters} clusters: {elapsed:.2f} seconds\")\n",
    "\n",
    "# Create a dictionary to hold the cluster centers\n",
    "word_centroid_map = dict(zip(model.wv.index_to_key, idx))\n",
    "\n",
    "# Print the first 10 clusters\n",
    "for cluster in range(10):\n",
    "    # Print the cluster number\n",
    "    print(f\"\\nCluster {cluster}:\")\n",
    "\n",
    "    # Get the words in the cluster\n",
    "    words = [word for word, idx in word_centroid_map.items() if idx == cluster]\n",
    "\n",
    "    # Print the words in the cluster\n",
    "    print(\" \".join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c4edb3",
   "metadata": {},
   "source": [
    "Now have centroid assignments for each word and can define a function to convert reviews into bags-of-centroids. Works similarly to bag-of-words but uses semantically-related clusters instead of individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "57c5a046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bag_of_centroids(words, word_centroid_map) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Converts a list of words into a bag of centroids.\n",
    "\n",
    "    Parameters:\n",
    "        words (list): A list of words to be converted into centroids.\n",
    "        word_centroid_map (dict): A dictionary mapping words to their corresponding cluster indices.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: A numpy array representing the bag of centroids for the words.\n",
    "    \"\"\"\n",
    "\n",
    "    # The number of clusters is equal to the highest cluster index\n",
    "    num_centroids = max(word_centroid_map.values()) + 1\n",
    "\n",
    "    # Initialize a bag of centroids with zeros\n",
    "    bag_of_centroids = np.zeros(num_centroids, dtype=\"float32\")\n",
    "\n",
    "    # Iterate through each word in the input list\n",
    "    for word in words:\n",
    "        # Check if the word is in the word_centroid_map\n",
    "        if word in word_centroid_map:\n",
    "            # Increment the corresponding centroid count\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f48bbf",
   "metadata": {},
   "source": [
    "Create bags of centroids for training and test set, then train a random forest and extract results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "abe1f670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating bags of centroids for training reviews...\n",
      "Processing review 1000 of 25000\n",
      "Processing review 2000 of 25000\n",
      "Processing review 3000 of 25000\n",
      "Processing review 4000 of 25000\n",
      "Processing review 5000 of 25000\n",
      "Processing review 6000 of 25000\n",
      "Processing review 7000 of 25000\n",
      "Processing review 8000 of 25000\n",
      "Processing review 9000 of 25000\n",
      "Processing review 10000 of 25000\n",
      "Processing review 11000 of 25000\n",
      "Processing review 12000 of 25000\n",
      "Processing review 13000 of 25000\n",
      "Processing review 14000 of 25000\n",
      "Processing review 15000 of 25000\n",
      "Processing review 16000 of 25000\n",
      "Processing review 17000 of 25000\n",
      "Processing review 18000 of 25000\n",
      "Processing review 19000 of 25000\n",
      "Processing review 20000 of 25000\n",
      "Processing review 21000 of 25000\n",
      "Processing review 22000 of 25000\n",
      "Processing review 23000 of 25000\n",
      "Processing review 24000 of 25000\n",
      "Processing review 25000 of 25000\n",
      "Creating bags of centroids for test reviews...\n",
      "Processing review 1000 of 25000\n",
      "Processing review 2000 of 25000\n",
      "Processing review 3000 of 25000\n",
      "Processing review 4000 of 25000\n",
      "Processing review 5000 of 25000\n",
      "Processing review 6000 of 25000\n",
      "Processing review 7000 of 25000\n",
      "Processing review 8000 of 25000\n",
      "Processing review 9000 of 25000\n",
      "Processing review 10000 of 25000\n",
      "Processing review 11000 of 25000\n",
      "Processing review 12000 of 25000\n",
      "Processing review 13000 of 25000\n",
      "Processing review 14000 of 25000\n",
      "Processing review 15000 of 25000\n",
      "Processing review 16000 of 25000\n",
      "Processing review 17000 of 25000\n",
      "Processing review 18000 of 25000\n",
      "Processing review 19000 of 25000\n",
      "Processing review 20000 of 25000\n",
      "Processing review 21000 of 25000\n",
      "Processing review 22000 of 25000\n",
      "Processing review 23000 of 25000\n",
      "Processing review 24000 of 25000\n",
      "Processing review 25000 of 25000\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to hold the bags of centroids for the training reviews\n",
    "train_centroids = np.zeros((train[\"review\"].size, num_clusters), dtype=\"float32\")\n",
    "\n",
    "# Transform training reviews into bags of centroids\n",
    "print(\"Creating bags of centroids for training reviews...\")\n",
    "for i, review in enumerate(clean_train_reviews):\n",
    "    if (i + 1) % 1000 == 0:\n",
    "        print(f\"Processing review {i + 1} of {len(clean_train_reviews)}\")\n",
    "    \n",
    "    # Create a bag of centroids for the review\n",
    "    train_centroids[i] = create_bag_of_centroids(review, word_centroid_map)\n",
    "\n",
    "# Initialize an empty list to hold the bags of centroids for the test reviews\n",
    "test_centroids = np.zeros((test[\"review\"].size, num_clusters), dtype=\"float32\")\n",
    "\n",
    "# Transform test reviews into bags of centroids\n",
    "print(\"Creating bags of centroids for test reviews...\")\n",
    "for i, review in enumerate(clean_test_reviews):\n",
    "    if (i + 1) % 1000 == 0:\n",
    "        print(f\"Processing review {i + 1} of {len(clean_test_reviews)}\")\n",
    "    \n",
    "    # Create a bag of centroids for the review\n",
    "    test_centroids[i] = create_bag_of_centroids(review, word_centroid_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "35fda42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the Random Forest classifier to the training data using bags of centroids...\n",
      "Predicting sentiment for the test data using bags of centroids...\n"
     ]
    }
   ],
   "source": [
    "# Fit a Random Forest classifier to the training data using the bags of centroids\n",
    "forest = RandomForestClassifier(n_estimators=100)\n",
    "print(\"Fitting the Random Forest classifier to the training data using bags of centroids...\")\n",
    "forest = forest.fit(train_centroids, train[\"sentiment\"])\n",
    "\n",
    "# Predict sentiment for the test data using the trained Random Forest classifier\n",
    "print(\"Predicting sentiment for the test data using bags of centroids...\")\n",
    "result = forest.predict(test_centroids)\n",
    "\n",
    "# Copy results to a DataFrame for submission\n",
    "output = pd.DataFrame(data={\"id\": test[\"id\"], \"sentiment\": result})\n",
    "\n",
    "# Write the DataFrame to a CSV file for submission\n",
    "output.to_csv(\"../data/bag_of_centroids_submission.csv\", index=False, quoting=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
